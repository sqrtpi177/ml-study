{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFeO_Lm8looa"
   },
   "source": [
    "Multi Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIPMZ-Hskwn_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ar19VPrmaSO"
   },
   "outputs": [],
   "source": [
    "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rN4rYfVkmcoq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSmw3snbntAF"
   },
   "outputs": [],
   "source": [
    "flags = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvGa2khsmeOu"
   },
   "outputs": [],
   "source": [
    "# doesn't work often(maybe because of timeout?)\n",
    "# def simple_map_fn(index, flags):\n",
    "#     torch.manual_seed(1234)\n",
    "#     device = xm.xla_device()\n",
    "#     t = torch.randn((2, 2), device=device)\n",
    "#     print(\"Process\", index, \"is using\", xm.xla_real_devices([str(device)])[0])\n",
    "#     xm.rendezvous('init')\n",
    "\n",
    "# flags = {}\n",
    "# xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOYxvrtgmgwi"
   },
   "outputs": [],
   "source": [
    "# again doesn't work well\n",
    "# def simple_map_fn(index, flags):\n",
    "#     torch.manual_seed(1234)\n",
    "#     device = xm.xla_device()\n",
    "#     t = torch.randn((2, 2), device=device)\n",
    "#     out = str(t)\n",
    "\n",
    "#     if xm.is_master_ordinal():\n",
    "#         print(out)\n",
    "    \n",
    "#     xm.rendezvous('init')\n",
    "\n",
    "# xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j16sbDpVmiY2"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IReB7yuvmluh"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import time\n",
    "\n",
    "def map_fn(index, flags):\n",
    "  ## Setup \n",
    "\n",
    "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
    "  torch.manual_seed(flags['seed'])\n",
    "\n",
    "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
    "  device = xm.xla_device()  \n",
    "\n",
    "\n",
    "  ## Dataloader construction\n",
    "\n",
    "  # Creates the transform for the raw Torchvision data\n",
    "  # See https://pytorch.org/docs/stable/torchvision/models.html for normalization\n",
    "  # Pre-trained TorchVision models expect RGB (3 x H x W) images\n",
    "  # H and W should be >= 224\n",
    "  # Loaded into [0, 1] and normalized as follows:\n",
    "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "  to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
    "  resize = transforms.Resize((224, 224))\n",
    "  my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
    "\n",
    "  # Downloads train and test datasets\n",
    "  # Note: master goes first and downloads the dataset only once (xm.rendezvous)\n",
    "  #   all the other workers wait for the master to be done downloading.\n",
    "\n",
    "  if not xm.is_master_ordinal():\n",
    "    xm.rendezvous('download_only_once')\n",
    "\n",
    "  train_dataset = datasets.FashionMNIST(\n",
    "    \"/tmp/fashionmnist\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=my_transform)\n",
    "\n",
    "  test_dataset = datasets.FashionMNIST(\n",
    "    \"/tmp/fashionmnist\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=my_transform)\n",
    "  \n",
    "  if xm.is_master_ordinal():\n",
    "    xm.rendezvous('download_only_once')\n",
    "  \n",
    "  # Creates the (distributed) train sampler, which let this process only access\n",
    "  # its portion of the training dataset.\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset,\n",
    "    num_replicas=xm.xrt_world_size(),\n",
    "    rank=xm.get_ordinal(),\n",
    "    shuffle=True)\n",
    "  \n",
    "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    test_dataset,\n",
    "    num_replicas=xm.xrt_world_size(),\n",
    "    rank=xm.get_ordinal(),\n",
    "    shuffle=False)\n",
    "  \n",
    "  # Creates dataloaders, which load data in batches\n",
    "  # Note: test loader is not shuffled or sampled\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=flags['batch_size'],\n",
    "      sampler=train_sampler,\n",
    "      num_workers=flags['num_workers'],\n",
    "      drop_last=True)\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=flags['batch_size'],\n",
    "      sampler=test_sampler,\n",
    "      shuffle=False,\n",
    "      num_workers=flags['num_workers'],\n",
    "      drop_last=True)\n",
    "  \n",
    "\n",
    "  ## Network, optimizer, and loss function creation\n",
    "\n",
    "  # Creates AlexNet for 10 classes\n",
    "  # Note: each process has its own identical copy of the model\n",
    "  #  Even though each model is created independently, they're also\n",
    "  #  created in the same way.\n",
    "  net = torchvision.models.alexnet(num_classes=10).to(device).train()\n",
    "\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "  ## Trains\n",
    "  train_start = time.time()\n",
    "  for epoch in range(flags['num_epochs']):\n",
    "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
    "    for batch_num, batch in enumerate(para_train_loader):\n",
    "      data, targets = batch \n",
    "\n",
    "      # Acquires the network's best guesses at each class\n",
    "      output = net(data)\n",
    "\n",
    "      # Computes loss\n",
    "      loss = loss_fn(output, targets)\n",
    "\n",
    "      # Updates model\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # Note: optimizer_step uses the implicit Cloud TPU context to\n",
    "      #  coordinate and synchronize gradient updates across processes.\n",
    "      #  This means that each process's network has the same weights after\n",
    "      #  this is called.\n",
    "      # Warning: this coordination requires the actions performed in each \n",
    "      #  process are the same. In more technical terms, the graph that\n",
    "      #  PyTorch/XLA generates must be the same across processes. \n",
    "      xm.optimizer_step(optimizer)  # Note: barrier=True not needed when using ParallelLoader \n",
    "\n",
    "  elapsed_train_time = time.time() - train_start\n",
    "  print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) \n",
    "\n",
    "\n",
    "  ## Evaluation\n",
    "  # Sets net to eval and no grad context \n",
    "  net.eval()\n",
    "  eval_start = time.time()\n",
    "  with torch.no_grad():\n",
    "    num_correct = 0\n",
    "    total_guesses = 0\n",
    "\n",
    "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
    "    for batch_num, batch in enumerate(para_train_loader):\n",
    "      data, targets = batch\n",
    "\n",
    "      # Acquires the network's best guesses at each class\n",
    "      output = net(data)\n",
    "      best_guesses = torch.argmax(output, 1)\n",
    "\n",
    "      # Updates running statistics\n",
    "      num_correct += torch.eq(targets, best_guesses).sum().item()\n",
    "      total_guesses += flags['batch_size']\n",
    "  \n",
    "  elapsed_eval_time = time.time() - eval_start\n",
    "  print(\"Process\", index, \"finished evaluation. Evaluation time was:\", elapsed_eval_time)\n",
    "  print(\"Process\", index, \"guessed\", num_correct, \"of\", total_guesses, \"correctly for\", num_correct/total_guesses * 100, \"% accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfFjLEuXmnOy"
   },
   "outputs": [],
   "source": [
    "flags['batch_size'] = 32\n",
    "flags['num_workers'] = 8\n",
    "flags['num_epochs'] = 1\n",
    "flags['seed'] = 1234\n",
    "\n",
    "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4AGhL2cmpfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNNU7+25YAabBpvHhofYhwN",
   "collapsed_sections": [],
   "name": "practice-tpu-03.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
